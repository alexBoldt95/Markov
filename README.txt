A program that generates Markov chains from a selected training text. A funny place to find this algorithm at
work is at www.reddit.com/r/SubredditSimulator. Where all of the content is generated by bots using Markov chains
for their commenting. The idea is to use Markov chains to have the bots communicate as closely as they can to real
internet users.
 
***INSTRUCTIONS***
Run the MarkovMain class
In the window, go to the file menu and choose 'Open File' 
Choose a .txt file in the 'data' folder
In the input bar, input two integers separated by a space.
Press 'GO'
The first integer corresponds to how many words are looked at for each markov chain. The higher this number
is, the closer the generated text will look to its training text.
The second integer is simply how many words to print out everytime 'GO' is pressed.

The first time 'GO' is called, the program scans the entire file and creates its markov chain structures.
After that, the text creation should be very fast. Running the program for the first time on kjv10.txt (King
James Bible) takes several seconds.
***

Name: Alex Boldt
NetID: apb34
Hours Spent: 7 hours
Consulted With: Ademola Olayinka, Teddy Franceschi, David Brenes
Resources Used: Java 7/8 API  
Impressions: Pretty solid assignment, made you think a lot about classes, constructors, arrays and maps for sure.
----------------------------------------------------------------------
PART A: TIMING
PROBLEM A.1: 
alice.txt, k = 1, 200 characters max: 0.217s
alice.txt, k = 1, 400 characters max: 0.450s
alice.txt, k = 1, 800 characters max: 0.863s
alice.txt, k = 5, 200 characters max: 0.176s
alice.txt, k = 5, 400 characters max: 0.336s
alice.txt, k = 5, 800 characters max: 0.686s
alice.txt, k = 10, 200 characters max: 0.171s
alice.txt, k = 10, 400 characters max: 0.349s
alice.txt, k = 10, 800 characters max: 0.755s
hawthorne.txt, k = 1, 200 characters max: 0.707s
hawthorne.txt, k = 1, 400 characters max: 1.344s
hawthorne.txt, k = 1, 800 characters max: 2.859s

PROBLEM A.2: 2.Based on these results, what is the relationship between the
runtime and the length of the training text, the k-value, and the max
characters generated? How long do you think it will take to generate 1600
random characters using an order-5 Markov model when the The Complete Works
of William Shakespeare is used as the training text — our online copy of
this text contains roughly 5.5 million characters. Justify your answer —
don’t test empirically, use reasoning.

Running time seems to depend on amount of characters printed linearly (because the kGrams have to be generated
for every character) but the running time seems to be inversely related to the order k. Running time also 
seems to depend linearly on the length of the text, as more text has to be scanned each run for longer training files.


Alice in Wonderland has ~150000 characters, and the Complete Works has ~5.5 million, so about 35 times more. 
Alice with order 5 and 1600 chars runs about 1.416s, so the Complete Works with order 5 would take (35*1.416s) 
about 50 seconds.

PROBLEM A.3: Provide timings using your Map/Smart model for both creating
the map and generating 200, 400, 800, and 1600 character random texts with
an order-5 Model and alice.txt. Provide some explanation for the timings
you observe.

alice.txt, k = 5, 200 characters max: 0.275
alice.txt, k = 5, 400 characters max: 0.276
alice.txt, k = 5, 800 characters max: 0.276
alice.txt, k = 5, 1600 characters max: 0.274

It seems that the amount of characters that are printed doesn't change the running time. This is because
the printing of characters is trivial as the program always has access to the kGram map. It only needs 
to access the map and return what the map presents as the next kGram, and this takes very little time. Most
of the time for each run goes into creating the map, this is the only resource intensive part.

PROBLEM A.4: Provide timings for the WordMarkovModel with different
hashcode methods. Time the method you are given and compare the results
that you achieve with the better hashcode method that you developed.

alice.txt, k = 5, 1600 characters max, basic hashcode:  0.615s
alice.txt, k = 5, 1600 characters max, better hashcode: 0.218s


PROBLEM A.5: Using a k of your choice and clinton-nh.txt as the training
text, if we do not set a maximum number of characters generated (you can
effectively do this by having maxLetters be a large number, e.g. 1000 times
the size of the training text) what is the average number of characters
generated by our Markov text generator?

With a k of 3 and maxLetters of 1,000,000: generated 32 values: 
 {11064, 136, 905, 8537, 7669, 16655, 7688,
  3042, 1944, 644, 3975, 732, 17429, 5621, 509, 1618,
  8623, 2903, 5779, 2641, 31781, 9078, 937, 8618, 
  17821, 322, 53, 14928, 730, 4229, 2947, 16422}
  
  The average of this list to the nearest integer is 6749 characters (from Mathematica).

----------------------------------------------------------------------
PART B: HASHMAP VS. TREEMAP FOR WORDMARKOVMODEL
What are the differences in performance between HashMap and TreeMap as the
number of NGrams in the map grow?

For K=3 and maxLetters (words) 10000.

Graphing the running time of HashMaps and treeMaps versus the size of the WordNgrams generated from text
files of varying sizes returned two logarithmic graphs. The amount of keys were found by using 
the line: **System.out.println(markovMap.keySet().size());** The graph of TreeMaps running time is about
a factor of 6 higher than the HashMap running times. This means that TreeMaps take more running time 
than HashMaps per WordNgram generated from the text. 

This analysis was done on all of the given text files.

An image of the graphs can be found at: http://imgur.com/gallery/w0WAzc2

The dashed line corresponds to HashMap data, and the solid line to TreeMap data.

Data generated:
HashMaps:
Keys    Time(s)
{24035, 0.007},
{833, 0.000},
{756, 0.000},
{80509, 0.010},
{527454, 0.012},
{13903, 0.006},
{1043, 0.001},
{2296, 0.002},
{25182, 0.008}

TreeMaps:
Keys    Time(s)
{24035, 0.033},
{833, 0.007},
{756, 0.004},
{80509, 0.045},
{527454, 0.063},
{13903, 0.015},
{1043, 0.005},
{2296, 0.003},
{25182, 0.026}

